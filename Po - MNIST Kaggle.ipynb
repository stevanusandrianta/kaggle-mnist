{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784)\n",
      "(42000, 10)\n",
      "Training with normal neural network\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 25,818\n",
      "Trainable params: 25,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/10\n",
      "33600/33600 [==============================] - 2s - loss: 5.2115 - acc: 0.6485 - val_loss: 2.9609 - val_acc: 0.7861\n",
      "Epoch 2/10\n",
      "33600/33600 [==============================] - 2s - loss: 2.6274 - acc: 0.8011 - val_loss: 2.3853 - val_acc: 0.8052\n",
      "Epoch 3/10\n",
      "33600/33600 [==============================] - 1s - loss: 2.2709 - acc: 0.8162 - val_loss: 2.1567 - val_acc: 0.8157\n",
      "Epoch 4/10\n",
      "33600/33600 [==============================] - 1s - loss: 1.2135 - acc: 0.8329 - val_loss: 0.6623 - val_acc: 0.8656\n",
      "Epoch 5/10\n",
      "33600/33600 [==============================] - 1s - loss: 0.5126 - acc: 0.8895 - val_loss: 0.4752 - val_acc: 0.8999\n",
      "Epoch 6/10\n",
      "33600/33600 [==============================] - 1s - loss: 0.4031 - acc: 0.9082 - val_loss: 0.4339 - val_acc: 0.9146\n",
      "Epoch 7/10\n",
      "33600/33600 [==============================] - 1s - loss: 0.3463 - acc: 0.9183 - val_loss: 0.4325 - val_acc: 0.9151\n",
      "Epoch 8/10\n",
      "33600/33600 [==============================] - 1s - loss: 0.3086 - acc: 0.9264 - val_loss: 0.4056 - val_acc: 0.9139\n",
      "Epoch 9/10\n",
      "33600/33600 [==============================] - 1s - loss: 0.2703 - acc: 0.9336 - val_loss: 0.3521 - val_acc: 0.9274\n",
      "Epoch 10/10\n",
      "33600/33600 [==============================] - 1s - loss: 0.2518 - acc: 0.9370 - val_loss: 0.3252 - val_acc: 0.9298\n"
     ]
    }
   ],
   "source": [
    "#TRAINING DATA\n",
    "training_image = train.ix[:,1:].values\n",
    "print(training_image.shape)\n",
    "\n",
    "#LABEL\n",
    "training_label = train.ix[:,0].values\n",
    "training_label = to_categorical(training_label)\n",
    "print(training_label.shape)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Dropout\n",
    "\n",
    "print(\"Training with normal neural network\")\n",
    "nn_model=Sequential()\n",
    "nn_model.add(Dense(32,activation='relu',input_dim=(28*28)))\n",
    "nn_model.add(Dense(16,activation='relu'))\n",
    "nn_model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "nn_model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "print(nn_model.summary())\n",
    "nn_model_result = nn_model.fit(training_image, training_label, validation_split=0.2, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'training_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dd8522a93d45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtraining_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_image' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "training_image = training_image.reshape(training_image.shape[0], 28, 28, 1)\n",
    "print(training_image.shape)\n",
    "\n",
    "print(\"Training with convolution neural network\")\n",
    "cnn_model=Sequential()\n",
    "cnn_model.add(Conv2D(16, kernel_size=(3,3), activation='relu', input_shape=(28,28,1)))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "cnn_model.add(Conv2D(32, kernel_size=(2,2), activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "cnn_model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(64, activation='relu'))\n",
    "cnn_model.add(Dropout(0.2))\n",
    "cnn_model.add(Dense(32, activation='relu'))\n",
    "cnn_model.add(Dropout(0.2))\n",
    "cnn_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "cnn_model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "print(cnn_model.summary())\n",
    "cnn_model_result = cnn_model.fit(training_image, training_label, epochs=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27936/28000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "# #Predict with NN Model\n",
    "# test_image = test.values\n",
    "# nn_prediction = nn_model.predict_classes(test_image)\n",
    "# nn_result = pd.DataFrame({\"ImageId\": list(range(1,len(nn_prediction)+1)),\n",
    "#                           \"Label\": nn_prediction})\n",
    "# nn_result.to_csv(\"nn_output.csv\", index=False, header=True)\n",
    "\n",
    "#Predict with CNN Model\n",
    "test_image = test_image.reshape(test.shape[0],28,28,1)\n",
    "cnn_prediction = cnn_model.predict_classes(test_image)\n",
    "cnn_result = pd.DataFrame({\"ImageID\": list(range(1,len(cnn_prediction)+1)), \n",
    "                           \"Label\" : cnn_prediction})\n",
    "cnn_result.to_csv(\"cnn_output.csv\", index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
